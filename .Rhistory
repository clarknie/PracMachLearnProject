3 +c(-1,1)*qt(.975, 99)*.11
round(pbinom(q=54, size = 100, prob = 0.5, lower.tail = F), 4)
ppois(15800/3, 300, lower.tail = F)
ppois(15800/30, 300, lower.tail = F)
ppois(15800-1, 300*30, lower.tail = F)
15800/30
ppois(15800/30, 520, lower.tail = F)
ppois(15800-1, 520*30, lower.tail = F)
2*pnorm(-sqrt(50)/4)
power.t.test(n = 100, delta = 1, sd = sqrt(0.1), type = "one.sample", alternative = "one.sided")
power.t.test(n = 100, delta = 1, sd = .4, type = "one.sample", alternative = "one.sided")
power.t.test(n = 100, delta = 1, sd = 4, type = "one.sample", alternative = "one.sided")
power.t.test(delta = 0.01, sd = 0.04, power = 0.8, type = "one.sample", alternative = "one.sided")
power.t.test(delta = -0.01, sd = 0.04, power = 0.8, type = "one.sample", alternative = "one.sided")
power.t.test(delta = 0.01, sd = 0.04, power = 0.8, type = "one.sample", alternative = "one.sided")
power.t.test(delta = 0.01, sd = 0.05, power = 0.8, type = "one.sample", alternative = "one.sided")
x8 <- mtcars$mpg[mtcars$cyl==6]
x8 <- mtcars$mpg[mtcars$cyl==8]
x6 <- mtcars$mpg[mtcars$cyl==6]
t.test(x6, x8, var.equal = T)
z.test
x6
x8
nx6 <- length(x6)
nx8 <- length(x8)
sp68 <- sqrt(((nx6-1)*var(x6)+(nx8-1)*var(x8))/(nx6+nx8-2))
(mean(x6)-mean(x8))/(sp68*sqrt(1/nx6+1/nx8))
sp58
sp68
library(swirl)
swirl()
install_from_swirl("Statistical Inference")
swirl()
10/sqrt(100)
2
.8
15
qt(.95,15)
dim(fs)
t.test(fs$sheight-fs$fheight)
11.7885 * sd(fs$sheight-fs$fheight)/sqrt(1078)
mybin
7
8
pt(2.5, 15, lower.tail = F)
pt(2.5, 15, lower.tail = FALSE)
qnorm(.95)
qnorm(.99)
pnorm(2)
qnorm(2)
pnorm(2, lower.tail = FALSE)
mybin
pbinom(6,8,.5,lower.tail = FALSE)
pbinom(7,8,0.5)
ppois(9,5,lower.tail = FALSE)
myplot(34)
myplot(33.3)
myplot(30)
myplot(28)
z <- qnorm(.95)
pnorm(30+z,mean=30,lower.tail = FALSE)
pnorm(30+z,mean=32,lower.tail = FALSE)
pnorm(30+z,mean=32,sd = 1, lower.tail = FALSE)
pnorm(30+z*2,mean=32,sd = 2, lower.tail = FALSE)
power.t.test(n = 16,
| delta = 2 / 4, sd=1, type = "one.sample", alt = "one.sided")$power
power.t.test(n = 16, delta = 2 / 4, sd=1, type = "one.sample", alt = "one.sided")$power
power.t.test(n = 16, delta = 2 , sd=4, type = "one.sample", alt = "one.sided")$power
power.t.test(n = 16, delta = 100 , sd=200, type = "one.sample", alt = "one.sided")$power
power.t.test(power = .8, delta = 2 / 4, sd=1, type = "one.sample", alt
= "one.sided")$n
power.t.test(power = .8, delta = 2 , sd=4, type = "one.sample", alt
= "one.sided")$n
power.t.test(power = .8, delta = 100 , sd=200, type = "one.sample", alt
= "one.sided")$n
power.t.test(power = .8, n=26 , sd=1, type = "one.sample", alt
= "one.sided")$n
power.t.test(power = .8, n=26 , sd=1, type = "one.sample", alt
= "one.sided")$delta
power.t.test(power = .8, n=27 , sd=1, type = "one.sample", alt
= "one.sided")$delta
head(pValues)
sum(pValues<0.05)
sum(p.adjust(pValues, method = "bonferroni")<0.05)
sum(p.adjust(pValues, method = "BH")<0.05)
tail(trueStatus)
table(pValues2<0.05, trueStatus)
.48
24/500
table(p.adjust(pValues2, "bonferroni")<0.05, trueStatus)
table(p.adjust(pValues2, "BH")<0.05, trueStatus)
3.5
print(g2)
head(sh)
nh
median(resampledMeans)
median(resampledMedians)
median(sh)
sam <- sample(fh, nh*B, replace = TRUE)
resam <- matrix(sam, nrow = B)
dim(resam)
resam <- matrix(sam,  B, nh)
dim(resam)
meds <-apply(resam, 1, median)
median(fh)-median(meds)
sd(meds)
sd(resampledMedians)
quantile(resampledMedians, c(.025, .975))
quantile(meds, c(.025, .975))
dim(InsectSprays)
names(InsectSprays)
range(Bdata$count)
range(Cdata$count)
head(BCcounts)
BCcounts
group
testStat
obs <- testStat(BCcounts, group)
obs
mean(Bdata$count-Cdata$count)
sample(group)
perms
<- sapply(1 : 10000, function(i) testStat(BCcounts, sample(group)))
perms<- sapply(1 : 10000, function(i) testStat(BCcounts, sample(group)))
mean(perms > obs)
testStat(DEcounts, group)
perms <- sapply(1:10000, function(i) testStat(DEcounts, sample(group)))
install_from_swirl("Regression Models")
swirl()
fit <- lm(y~x,out2)
plot(fit, which = 1)
fitno <- lm(y~x,out2[-1,])
plot(fitno, which = 1)
coef(fit)-coef(fitno)
View(dfbeta(fit))
resno <- out2[1, "y"] - predict(fitno, out2[1,])
1-resid(fit)[1]/resno
View(hatvalues(fit))
sigma <- sqrt(sum(fit$residuals^2)/fit$df.residual)
rstd <- resid(fit)/(sigma*sqrt(1-hatvalues(fit)))
View(cbind(rstd, rstandard(fit)))
plot(fit, which = 3)
plot(fit, which = 2)
sigma1 <- sqrt(sd(fitno$residuals)/fitno$df.residual)
deviance(fitno)
?deviance
sd(fitno$residuals)
ssum(fitno$residuals^2)
sum(fitno$residuals^2)
sigma1 <- sqrt(sum(fitno$residuals^2)/fitno$df.residual)
resid(fit)[1]/(sigma1*sqrt(1-hatvalues(fit)[1]))
View(rstudent(fit))
dy <- predict(fitno, out2) - predict(fit, out2)
sum(dy^2)/(2*sigma^2)
plot(fit, which = 5)
rpg1()
source('~/Dropbox/code/R/vifSims.R')
rpg1()
makelms <- function(x1, x2, x3){
# Simulate a dependent variable, y, as x1
# plus a normally distributed error of mean 0 and
# standard deviation .3.
y <- x1 + rnorm(length(x1), sd = .3)
# Find the coefficient of x1 in 3 nested linear
# models, the first including only the predictor x1,
# the second x1 and x2, the third x1, x2, and x3.
c(coef(lm(y ~ x1))[2],
coef(lm(y ~ x1 + x2))[2],
coef(lm(y ~ x1 + x2 + x3))[2])
}
# Regressor generation process 1.
rgp1 <- function(){
print("Processing. Please wait.")
# number of samples per simulation
n <- 100
# number of simulations
nosim <- 1000
# set seed for reproducibility
set.seed(4321)
# Point A
x1 <- rnorm(n)
x2 <- rnorm(n)
x3 <- rnorm(n)
# Point B
betas <- sapply(1 : nosim, function(i)makelms(x1, x2, x3))
round(apply(betas, 1, var), 5)
}
rgp1()
rgp2()
head(swiss)
?swiss
mdl <- lm(Fertility ~ ., data = swiss)
vif(mdl)
mdl2 <- lm(Fertility ~ . - Education, data = swiss)
mdl2
dl2 <- lm(Fertility ~ . -Examination, swiss)
mdl2 <- lm(Fertility ~ . -Examination, swiss)
vif(mdl2)
x1c <- simbias()
apply(x1c, 1, mean)
fit1 <- lm(Fertility ~ Agriculture, swiss)
fit3 <- lm(Fertility ~ Agriculture + Examination + Education, swiss)
anova(fit1,fit3)
deviance(fit3)
d <- deviance(fit3)/43
n <- (deviance(fit1)-deviance(fit3))/2
n/d
pf(n/d, 2, 43, lower.tail = FALSE)
shapiro.test(fit3$residuals)
anova(fit1, fit3, fit5, fit6)
View(ravenData)
mdl <- glm(ravenWinNum ~ ravenScore, family = "binomial", data = ravenData)
lodds <- predict(mdl,
| data.frame(ravenScore=c(0, 3, 6)))
lodds <- predict(mdl,
data.frame(ravenScore=c(0, 3, 6)))
exp(lodds)/(1+exp(lodds))
summary(mdl)
confint(mdl)
exp(confint(mdl))
anova(mdl)
qchisq(.95,1)
var(rpois(1000, 50))
nxt()
head(hits)
class(hits[,'date'])
class(hits[,1])
as.integer(head(hits[,'date']))
mdl <- glm(visits ~ date, poisson, hits)
summary(mdl)
exp(confint(mdl, 'date'))
which.max(hits[,'visits'])
hits[704,]
mdl$fitted.values[704]
lambda <- mdl$fitted.values[704]
qpois(.95, lambda)
mdl2 <- glm(visits ~ date, poisson, hits offset = log(visits + 1))
mdl2 <- glm(visits ~ date, poisson, hits, offset = log(visits + 1))
mdl2 <- glm(simplystats ~ date, poisson, hits, offset = log(visits + 1))
summary(mdl2)
qpois(.95, mdl2$fitted.values[704])
glm2 <- glm(gaData$simplystats ~ julian(gaData$date),offset=log(visits+1),
family="poisson",data=gaData)
glm2$fitted.values
18.16/30
methods(colSums)
methods("colSums")
methods("show")
methods("lm")
methods("dgamma	")
methods("dgamma")
methods("predict")
library(ElemStatLearn)
library(caret)
data(vowel.train)
data(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
mod1 <- train(y ~ ., method = "rf", data = vowel.train)
predict(mod1,vowel.test)
mod1_yhat <- predict(mod1,vowel.test)
confusionMatrix(mod1_yhat,vowel.test$y)
mod2 <- train(y ~ ., method = "gbm", data = vowel.train)
mod2_yhat <- predict(mod2,vowel.test)
confusionMatrix(mod2_yhat,vowel.test$y)
agreedIndex <- mod1_yhat == mod2_yhat
confusionMatrix(mod2_yhat[agreedIndex], vowel.test$y[agreedIndex])
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
mod21 <- train(diagnosis ~ ., method = "rf", data = training)
set.seed(62433)
mod21 <- train(diagnosis ~ ., method = "rf", data = training)
mod22 <- train(diagnosis ~ ., method = "gbm", data = training)
mod23 <- train(diagnosis ~ ., method = "lda", data = training)
pred21 <- predict(mod21, testing)
pred22 <- predict(mod22, testing)
pred23 <- predict(mod23, testing)
predDF2 <- data.frame(pred21, pred22, pred23, diagnosis=testing$diagnosis)
combMod2 <- train(diagnosis ~ ., method = "rf", data = predDF2)
predc2 <- predict(combMod2, testing)
confusionMatrix(predc2, testing$diagnosis)
confusionMatrix(pred21, testing$diagnosis)
confusionMatrix(pred22, testing$diagnosis)
confusionMatrix(pred23, testing$diagnosis)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
?plot.enet
??plot.enet
mod31 <- train(CompressiveStrength ~., method = "lasso", data = training)
mod31 <- train(CompressiveStrength ~., method = "lasso", data = training)
??plot.enet
plot.enet(mod31)
plot(mod31)
plot(mod31, xvar=step)
plot(mod31, xvar="step")
plot(mod31, xvar="step", use.color = TRUE)
plot(mod31, xvar="penalty", use.color = TRUE)
plot(mod31$finalModel, xvar="penalty", use.color = TRUE)
library(lubridate)  # For year() function below
dat = read.csv("/Users/chengnie/Dropbox/code/R/coursera/08_PracticalMachineLearning/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
library(forecast)
install.packages("forecast", lib="/Users/chengnie/Library/R/3.2/library")
?bats
library(forecast)
?bats
tstest = ts(testing$visitsTumblr)
mod4 <- bats(tstrain)
plot(forecast(mod4))
plot(mod4)
h <- dim(testing)[1]
h
fcast <- forecast(fit, level = 95, h = h)
fcast <- forecast(mod4, level = 95, h = h)
accuracy(fcast, testing$visitsTumblr)
fcast
class(fcast)
l <- length(fcast$lower)
l
result <- c()
result <- c()
result <- c()
for (i in 1:h){
# x <- testing$visitsTumblr[i]
x <- tstest$visitsTumblr[i]
a <- fcast$lower[i] < x & x < fcast$upper[i]
result <- c(result, a)
}
for (i in 1:h){
# x <- testing$visitsTumblr[i]
x <- tstest[i]
a <- fcast$lower[i] < x & x < fcast$upper[i]
result <- c(result, a)
}
sum(result)/h * 100
library(e1071)
?e1071
??e1071
mod5 <- svm(CompressiveStrength ~., data = training)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
library(e1071)
set.seed(325)
mod5 <- svm(CompressiveStrength ~., data = training)
accuracy(predict(mod5, testing),testing$CompressiveStrength)
library(caret)
setwd("/Users/chengnie/Dropbox/code/R/coursera/08_PracticalMachineLearning/Project/PracMachLearnProject")
training_raw <- read.csv("./pml-training.csv")
testing_raw_raw <- read.csv("./pml-testing.csv")
str(training_raw)
dim(training_raw)
str(testing_raw)
dim(testing_raw)
table(training_raw$user_name)
summary(training_raw$X)
table(training_raw$classe)
table(training_raw$user_name,training_raw$classe)
table(testing_raw$user_name)
table(testing_raw$classe)
# column 160 is different for training and testing
testing_raw$problem_id
sum(is.na(training_raw$raw_timestamp_part_1))
# http://stackoverflow.com/questions/8317231/elegant-way-to-report-missing-values-in-a-data-frame
apply(is.na(training_raw), 2, sum)
trainCols <- colSums(is.na(training_raw))==0
testCols <- colSums(is.na(testing_raw))==0
setwd("/Users/chengnie/Dropbox/code/R/coursera/08_PracticalMachineLearning/Project/PracMachLearnProject")
training_raw <- read.csv("./pml-training.csv")
testing_raw_raw <- read.csv("./pml-testing.csv")
trainCols <- colSums(is.na(training_raw))==0
testCols <- colSums(is.na(testing_raw))==0
testing_raw <- read.csv("./pml-testing.csv")
testCols <- colSums(is.na(testing_raw))==0
sum(trainCols)
sum(testCols)
names(testCols)
names(trainCols)
names(testCols) == names(trainCols)
sum(names(testCols) == names(trainCols))
keepCols <- trainCols & testCols
keepCols
sum(keepCols)
keepCols[1] <- FALSE
dim(training_raw[,keepCols])
sum(is.na(testing_raw[,keepCols]))
head(testing_raw[,keepCols])
colSums(is.na(testing_raw[,keepCols]))
colSums(is.na(training_raw[,keepCols]))
training <- training_raw[,keepCols]
testing <- testing[,keepCols]
set.seed(123)
fit1 <- train(classe ~ ., method = "rpart", data = training[, keepCols])
training <- training_raw[,keepCols]
testing <- testing[,keepCols]
testing <- testing_raw[,keepCols]
fit1 <- train(classe ~ ., method = "rpart", data = training[, keepCols])
fit1 <- train(classe ~ ., method = "rpart", data = training)
fit1$finalModel
predict(fit1)
keepCols[1:7] <- FALSE
training1 <- training_raw[,keepCols]
validation <- testing_raw[,keepCols]
inTrain <- createDataPartition(y=training1$classe, p=0.6, list=FALSE )
training2 <- training1[inTrain,]
testing2 <- training1[-inTrain,]
fitControl <- trainControl(## 10-fold CV
method = "repeatedcv",
number = 10,
## repeated one time
repeats = 10)
fit3 <- train(classe ~ . ,data = training2, method = "rf", ntree = 3, trControl = fitControl, verbose = FALSE)
predict(fit3, validation)
answers = c("B","A","B","A","A","E","B","D","A","A",
"B","C","B","A","E","E","A","B","B","B")
confusionMatrix(predict(fit3, validation),answers)
300/50
library(xgboost)
install.packages("xgboost", lib="/Users/chengnie/Library/R/3.2/library")
library(xgboost)
library(readr)
library(stringr)
library(caret)
library(car)
set.seed(1)
df_train = read_csv("/Users/chengnie/Documents/data/airbnb_Data/train_users_2.csv")
df_test = read_csv("/Users/chengnie/Documents/data/airbnb_Data/test_users.csv")
labels = df_train['country_destination']
dim(df_train)
df_train = df_train[-grep('country_destination', colnames(df_train))]
dim(df_train)
dim(labels)
head(labels)
df_all = rbind(df_train,df_test)
df_all = df_all[-c(which(colnames(df_all) %in% c('date_first_booking')))]
sum(is.na(df_all))
df_all[is.na(df_all)] <- -1
?str_split_fixed
head(df_all$date_account_created)
str_split_fixed(head(df_all$date_account_created), '-', 3)
dac = as.data.frame(str_split_fixed(df_all$date_account_created, '-', 3))
df_all['dac_year'] = dac[,1]
df_all['dac_month'] = dac[,2]
df_all['dac_day'] = dac[,3]
df_all = df_all[,-c(which(colnames(df_all) %in% c('date_account_created')))]
df_all[,'tfa_year'] = substring(as.character(df_all[,'timestamp_first_active']), 1, 4)
df_all[,'tfa_month'] = substring(as.character(df_all['timestamp_first_active']), 5, 6)
df_all[,'tfa_day'] = substring(as.character(df_all['timestamp_first_active']), 7, 8)
df_all = df_all[,-c(which(colnames(df_all) %in% c('timestamp_first_active')))]
df_all[df_all$age < 14 | df_all$age > 100,'age'] <- -1
ohe_feats = c('gender', 'signup_method', 'signup_flow', 'language', 'affiliate_channel', 'affiliate_provider', 'first_affiliate_tracked', 'signup_app', 'first_device_type', 'first_browser')
dummies <- dummyVars(~ gender + signup_method + signup_flow + language + affiliate_channel + affiliate_provider + first_affiliate_tracked + signup_app + first_device_type + first_browser, data = df_all)
df_all_ohe <- as.data.frame(predict(dummies, newdata = df_all))
df_all_combined <- cbind(df_all[,-c(which(colnames(df_all) %in% ohe_feats))],df_all_ohe)
X = df_all_combined[df_all_combined$id %in% df_train$id,]
y <- recode(labels$country_destination,"'NDF'=0; 'US'=1; 'other'=2; 'FR'=3; 'CA'=4; 'GB'=5; 'ES'=6; 'IT'=7; 'PT'=8; 'NL'=9; 'DE'=10; 'AU'=11")
??recode
library(car)
detach(caret)
detach("caret")
library(caret)
detach("package:caret", unload = TRUE)
library(car)
library(caret)
y <- recode(labels$country_destination,"'NDF'=0; 'US'=1; 'other'=2; 'FR'=3; 'CA'=4; 'GB'=5; 'ES'=6; 'IT'=7; 'PT'=8; 'NL'=9; 'DE'=10; 'AU'=11")
X_test = df_all_combined[df_all_combined$id %in% df_test$id,]
?xgboost
xgb <- xgboost(data = data.matrix(X[,-1]),
label = y,
eta = 0.1,
max_depth = 9,
nround=25,
subsample = 0.5,
colsample_bytree = 0.5,
seed = 1,
eval_metric = "merror",
objective = "multi:softprob",
num_class = 12,
nthread = 3
)
y_pred <- predict(xgb, data.matrix(X_test[,-1]))
head(y_pred)
dim(y_pred)
length(y_pred)
predictions <- as.data.frame(matrix(y_pred, nrow=12))
dim(predictions)
rownames(predictions) <- c('NDF','US','other','FR','CA','GB','ES','IT','PT','NL','DE','AU')
predictions_top5 <- as.vector(apply(predictions, 2, function(x) names(sort(x)[12:8])))
idx = X_test$id
id_mtx <-  matrix(idx, 1)[rep(1,5), ]
ids <- c(id_mtx)
submission <- NULL
submission$id <- ids
submission$country <- predictions_top5
submission <- as.data.frame(submission)
write.csv(submission, "/Users/chengnie/Documents/data/airbnb_Data/submission.csv", quote=FALSE, row.names = FALSE)
